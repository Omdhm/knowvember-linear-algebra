{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907de2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports - Just NumPy and Matplotlib!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nice inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Ready to build Linear Regression from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c31d58",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Generate Some Data\n",
    "\n",
    "Let's create a simple dataset where we **know** the true relationship:\n",
    "\n",
    "$$y = 2.5x + 3 + \\text{noise}$$\n",
    "\n",
    "Our goal: Can we recover the slope (2.5) and intercept (3) from the noisy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé≤ Generate synthetic data\n",
    "# True parameters (what we want to discover)\n",
    "# Generate random X values\n",
    "# Generate Y with some noise\n",
    " # Mean=0, Std=2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Generated {n_samples} data points\")\n",
    "print(f\"   X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"   y range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5718824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, c='#007079', s=60, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "plt.xlabel('X (Feature)', fontsize=12)\n",
    "plt.ylabel('y (Target)', fontsize=12)\n",
    "plt.title('Our Dataset: Can you see the linear trend?', fontsize=14)\n",
    "\n",
    "# Draw the true line (hidden from our model)\n",
    "X_line = np.linspace(0, 10, 100)\n",
    "y_true = TRUE_SLOPE * X_line + TRUE_INTERCEPT\n",
    "plt.plot(X_line, y_true, 'g--', linewidth=2, alpha=0.5, label=f'True: y = {TRUE_SLOPE}x + {TRUE_INTERCEPT}')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536b92b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding the Problem\n",
    "\n",
    "We want to find the **best line** that fits the data:\n",
    "\n",
    "$$\\hat{y} = w_1 \\cdot x + w_0$$\n",
    "\n",
    "Where:\n",
    "- $w_1$ = slope (weight)\n",
    "- $w_0$ = intercept (bias)\n",
    "\n",
    "**\"Best\"** means minimizing the **Sum of Squared Errors (SSE)**:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (\\text{error}_i)^2$$\n",
    "\n",
    "Where each **error** (or residual) is the vertical distance between the actual value and our prediction:\n",
    "\n",
    "$$\\text{error}_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "We **square** the errors so that:\n",
    "1. Positive and negative errors don't cancel out\n",
    "2. Larger errors are penalized more heavily\n",
    "\n",
    "Let's visualize what \"error\" means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the \"errors\" (residuals)\n",
    "# A bad guess for our line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65588baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, c='#007079', s=60, alpha=0.7, edgecolors='white', label='Data')\n",
    "\n",
    "# Draw the bad line\n",
    "plt.plot(X_line, guessed_slope * X_line + guessed_intercept, 'r-', linewidth=2, label=f'Bad Guess: y = {guessed_slope}x + {guessed_intercept}')\n",
    "\n",
    "# Draw vertical lines showing errors\n",
    "for i in range(len(X)):\n",
    "    plt.plot([X[i], X[i]], [y[i], y_pred_bad[i]], 'r-', alpha=0.3, linewidth=1)\n",
    "\n",
    "# Calculate SSE\n",
    "sse_bad = np.sum((y - y_pred_bad) ** 2)\n",
    "plt.title(f'Visualizing Errors (Residuals)\\nSum of Squared Errors = {sse_bad:.2f}', fontsize=14)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791431a",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Math - Deriving the Normal Equation\n",
    "\n",
    "Instead of guessing, we can solve for the **optimal** weights directly using Linear Algebra!\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "We rewrite our problem in matrix form:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}$ is our **design matrix** (with a column of 1s for the bias)\n",
    "- $\\mathbf{w}$ is our weight vector $[w_0, w_1]^T$\n",
    "- $\\mathbf{y}$ is our target vector\n",
    "- $\\boldsymbol{\\varepsilon}$ is the error/noise\n",
    "\n",
    "### Deriving the Normal Equation\n",
    "\n",
    "**Goal:** Find $\\mathbf{w}$ that minimizes the Sum of Squared Errors (SSE).\n",
    "\n",
    "Remember, SSE is the sum of all squared errors:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (\\text{error}_i)^2$$\n",
    "\n",
    "In matrix notation, this becomes:\n",
    "\n",
    "$$\\text{SSE} = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$$\n",
    "\n",
    "> üí° The $\\|\\cdot\\|^2$ notation means \"squared length\" of a vector, which equals the sum of squared components.\n",
    "\n",
    "**Step 1: Expand the expression**\n",
    "\n",
    "Using the rule $(a - b)^T(a - b) = a^Ta - a^Tb - b^Ta + b^Tb$:\n",
    "\n",
    "$$\\text{SSE} = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "Since $\\mathbf{y}^T\\mathbf{X}\\mathbf{w}$ is a scalar (a single number), it equals its transpose: $\\mathbf{w}^T\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "$$\\text{SSE} = \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "**Step 2: Take the derivative with respect to $\\mathbf{w}$**\n",
    "\n",
    "To find the minimum, we take the derivative and set it to zero. Using matrix calculus rules:\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{w}^T\\mathbf{a}) = \\mathbf{a}$\n",
    "- $\\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{w}^T\\mathbf{A}\\mathbf{w}) = 2\\mathbf{A}\\mathbf{w}$ (when $\\mathbf{A}$ is symmetric)\n",
    "\n",
    "$$\\frac{\\partial \\text{SSE}}{\\partial \\mathbf{w}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "**Step 3: Set derivative to zero and solve**\n",
    "\n",
    "At the minimum, the gradient equals zero:\n",
    "\n",
    "$$-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\mathbf{w} = \\mathbf{0}$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\\mathbf{X}^T\\mathbf{X}\\mathbf{w} = \\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Step 4: Solve for $\\mathbf{w}$**\n",
    "\n",
    "Multiply both sides by $(\\mathbf{X}^T\\mathbf{X})^{-1}$:\n",
    "\n",
    "$$\\boxed{\n",
    "    {\\mathbf{w}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}}$$\n",
    "\n",
    "This is the **Normal Equation** ‚Äî the closed-form solution that gives us the weights minimizing the sum of squared errors! üéØ\n",
    "\n",
    "Let's implement this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf63c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Step 3a: Create the Design Matrix\n",
    "# Our X is currently shape (n_samples,)\n",
    "# We need to add a column of 1s for the bias term\n",
    "# Reshape X to be a column vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e898e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Design Matrix X:\")\n",
    "print(f\"   Shape: {X_design.shape}\")\n",
    "print(f\"   First 5 rows:\")\n",
    "print(X_design[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495effe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Step 3b: Compute X^T X (the Gram Matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Gram Matrix (X^T X):\")\n",
    "print(f\"   Shape: {XtX.shape}\")\n",
    "print(f\"   Values:\")\n",
    "print(XtX)\n",
    "print(f\"\\n   Determinant: {np.linalg.det(XtX):.2f}\")\n",
    "print(f\"   ‚úÖ Determinant ‚â† 0, so matrix is invertible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Step 3c: Compute X^T y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Step 3d: Solve the Normal Equation!\n",
    "\n",
    "# Method 1: Direct inverse (less stable numerically)\n",
    "# w = np.linalg.inv(XtX) @ Xty\n",
    "\n",
    "# Method 2: Using np.linalg.solve (more stable)\n",
    "# This solves: XtX @ w = Xty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ SOLUTION FOUND!\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   Learned Intercept (w‚ÇÄ): {learned_intercept:.4f}\")\n",
    "print(f\"   Learned Slope (w‚ÇÅ):     {learned_slope:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   True Intercept: {TRUE_INTERCEPT}\")\n",
    "print(f\"   True Slope:     {TRUE_SLOPE}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   Error in Intercept: {abs(learned_intercept - TRUE_INTERCEPT):.4f}\")\n",
    "print(f\"   Error in Slope:     {abs(learned_slope - TRUE_SLOPE):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42a14b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d63513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Plot our learned line vs the true line\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Data points\n",
    "plt.scatter(X, y, c='#007079', s=60, alpha=0.7, edgecolors='white', label='Data')\n",
    "\n",
    "# True line\n",
    "y_true = TRUE_SLOPE * X_line + TRUE_INTERCEPT\n",
    "plt.plot(X_line, y_true, 'g--', linewidth=2, alpha=0.7, \n",
    "         label=f'True: y = {TRUE_SLOPE}x + {TRUE_INTERCEPT}')\n",
    "\n",
    "# Learned line\n",
    "y_learned = learned_slope * X_line + learned_intercept\n",
    "plt.plot(X_line, y_learned, 'r-', linewidth=2, \n",
    "         label=f'Learned: y = {learned_slope:.2f}x + {learned_intercept:.2f}')\n",
    "\n",
    "plt.xlabel('X (Feature)', fontsize=12)\n",
    "plt.ylabel('y (Target)', fontsize=12)\n",
    "plt.title('Linear Regression: Learned vs True', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27474db",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the Model\n",
    "\n",
    "Let's calculate some metrics to see how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Model Evaluation\n",
    "# Predictions\n",
    "# Mean Squared Error (MSE)\n",
    "# Root Mean Squared Error (RMSE)\n",
    "# R¬≤ Score (Coefficient of Determination)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d081e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä MODEL EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   Mean Squared Error (MSE):  {mse:.4f}\")\n",
    "print(f\"   Root MSE (RMSE):           {rmse:.4f}\")\n",
    "print(f\"   R¬≤ Score:                  {r2:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nüí° R¬≤ = {r2:.2%} of variance explained by the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dbdd0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Compare with sklearn\n",
    "\n",
    "Let's verify our implementation matches sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sklearn model\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "print(\"üî¨ COMPARISON: Our Model vs sklearn\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<20} {'Ours':<15} {'sklearn':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Intercept (bias)':<20} {learned_intercept:<15.6f} {sklearn_model.intercept_:<15.6f}\")\n",
    "print(f\"{'Slope (weight)':<20} {learned_slope:<15.6f} {sklearn_model.coef_[0]:<15.6f}\")\n",
    "print(f\"{'R¬≤ Score':<20} {r2:<15.6f} {sklearn_model.score(X.reshape(-1,1), y):<15.6f}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n‚úÖ Our implementation matches sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843877d",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "A fully functional Linear Regression model using **only NumPy**!\n",
    "\n",
    "### The Math Behind It\n",
    "The **Normal Equation** gives us the closed-form solution:\n",
    "\n",
    "$$\\hat{\\mathbf{w}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "### Key Linear Algebra Concepts Used\n",
    "1. **Matrix Multiplication** ‚Äî Building $X^T X$ and $X^T y$\n",
    "2. **Matrix Inverse** ‚Äî Solving for weights\n",
    "3. **Determinant** ‚Äî Checking if solution exists (det ‚â† 0)\n",
    "\n",
    "### When to Use This vs Gradient Descent\n",
    "| Normal Equation | Gradient Descent |\n",
    "|-----------------|------------------|\n",
    "| ‚úÖ Exact solution | ‚ö†Ô∏è Approximate |\n",
    "| ‚úÖ No hyperparameters | ‚ö†Ô∏è Learning rate, iterations |\n",
    "| ‚ö†Ô∏è Slow for large n_features | ‚úÖ Scales well |\n",
    "| ‚ö†Ô∏è Needs invertible $X^T X$ | ‚úÖ Works even with collinearity |\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand the math behind sklearn's LinearRegression!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b58bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99ce2325",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
